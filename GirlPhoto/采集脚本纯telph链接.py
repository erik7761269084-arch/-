import os
import requests
from bs4 import BeautifulSoup
import re
import time
import json

# ================== é…ç½® ==================
TXT_FILE = r"D:\project\pythonLL\telegraph_links_exhentaiäº”æ˜Ÿæ¼«ç”»1.txt"

# åˆ†é¡µ JS è¾“å‡ºç›®å½•
OUTPUT_DIR_JS = r"E:\é¢‘é“ç®¡ç†å¤§å…¨\comics\category\all_page"
# æ¯ä¸ªæ¼«ç”»ç‹¬ç«‹ JS æ–‡ä»¶ç›®å½•
OUTPUT_COMICS_DIR = r"E:\é¢‘é“ç®¡ç†å¤§å…¨\comics\menu"

os.makedirs(OUTPUT_DIR_JS, exist_ok=True)
os.makedirs(OUTPUT_COMICS_DIR, exist_ok=True)

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Referer": "https://telegra.ph/"
}

# æ¯ä¸ªåˆ†é¡µæ–‡ä»¶åŒ…å«æ¼«ç”»æ•°é‡
ITEMS_PER_PAGE = 100

# åˆ†é¡µæ–‡ä»¶ç¼–å·èµ·å§‹å€¼ï¼ˆpage_1.js å¼€å§‹ï¼‰
START_PAGE_NUM = 2

# æ¼«ç”» IDcode èµ·å§‹å€¼ï¼ˆå¯æŒ‡å®šä»ä»»æ„æ•°å­—å¼€å§‹ï¼‰
START_IDCODE = 2

# ================== å·¥å…·å‡½æ•° ==================
def safe_filename(s):
    return re.sub(r'[\\/:*?"<>|]', '_', s)[:150].strip()

def normalize_src(u):
    if not u:
        return None
    u = u.strip()
    if u.startswith("//"):
        u = "https:" + u
    if u.startswith("/file/"):
        return "https://telegra.ph" + u
    if "telegra.ph/file/" in u and not u.startswith("http"):
        return "https://" + u
    return u

def extract_title(soup):
    tag = soup.select_one("header.tl_article_header h1") or soup.find("h1") or soup.find("title")
    return tag.get_text(strip=True) if tag else "æœªå‘½åæ¼«ç”»"

def extract_images(soup, html_text):
    imgs = []
    # ä¼˜å…ˆ figure
    for fig in soup.find_all("figure"):
        img = fig.find("img")
        if img:
            src = img.get("src") or img.get("data-src") or img.get("data-original")
            src = normalize_src(src)
            if src:
                imgs.append(src)
    # æ­£åˆ™å…œåº•
    if not imgs:
        pattern = re.compile(r'<img[^>]+src=["\']([^"\']+)["\']', re.I)
        for m in pattern.finditer(html_text):
            src = normalize_src(m.group(1))
            if src:
                imgs.append(src)
    # å»é‡ä¿æŒé¡ºåº
    seen = set()
    out = []
    for u in imgs:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out

# ================== ä¸»æµç¨‹ ==================
with open(TXT_FILE, "r", encoding="utf-8") as f:
    SOURCE_URLS = [line.strip() for line in f if line.strip()]

comic_list = []

for idx, url in enumerate(SOURCE_URLS):
    current_idcode = START_IDCODE + idx  # ä»æŒ‡å®šèµ·å§‹ IDcode å¼€å§‹
    print(f"é‡‡é›† IDcode {current_idcode}: {url}")
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = resp.apparent_encoding or "utf-8"
        html = resp.text
        soup = BeautifulSoup(html, "html.parser")

        title = extract_title(soup)
        imgs = extract_images(soup, html)
        cover = imgs[0] if imgs else ""

        # æ„å»ºæ¼«ç”»æ•°æ®å¯¹è±¡
        comic_data = {
            "IDcode": current_idcode,
            "title": title,
            "cover": cover,
            "url": f"https://yunvgong.com/comics/comicDetails.html?series={current_idcode}"
        }

        comic_list.append(comic_data)

        # æ¯ä¸ªæ¼«ç”»ç”Ÿæˆç‹¬ç«‹ JS æ–‡ä»¶
        comic_js_content = f"const comicData = {json.dumps({'IDcode': current_idcode, 'title': title, 'sourceUrl': url, 'images': imgs}, ensure_ascii=False, indent=4)};\n"
        comic_js_file = os.path.join(OUTPUT_COMICS_DIR, f"{current_idcode}_comics_data.js")
        with open(comic_js_file, "w", encoding="utf-8") as cf:
            cf.write(comic_js_content)

        print(f"ğŸ“„ å·²ç”Ÿæˆæ¼«ç”» JS æ–‡ä»¶: {comic_js_file}")
        time.sleep(0.5)
    except Exception as e:
        print(f"âŒ é‡‡é›†å¤±è´¥: {e}")
        continue

# ================== ç”Ÿæˆåˆ†é¡µ JS æ–‡ä»¶ ==================
page_count = (len(comic_list) + ITEMS_PER_PAGE - 1) // ITEMS_PER_PAGE

for page_idx in range(page_count):
    start = page_idx * ITEMS_PER_PAGE
    end = start + ITEMS_PER_PAGE
    page_items = comic_list[start:end]

    # å½“å‰åˆ†é¡µç¼–å·
    current_page_num = START_PAGE_NUM + page_idx

    # æ·»åŠ  pageSeq åºå·ï¼ˆä»å½“å‰ IDcode å¼€å§‹è¿ç»­å åŠ ï¼‰
    for i, item in enumerate(page_items):
        item["pageSeq"] = item["IDcode"]

    js_content = "// æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼štitleã€coverã€urlã€pageSeq\n"
    js_content += "const COMIC_DATA = " + json.dumps(page_items, ensure_ascii=False, indent=4) + ";\n"

    # åˆ†é¡µæ–‡ä»¶åç§°æŒ‰ç…§åºå·ç”Ÿæˆ page_1.js, page_2.js ...
    page_file = os.path.join(OUTPUT_DIR_JS, f"page_{current_page_num}.js")
    with open(page_file, "w", encoding="utf-8") as pf:
        pf.write(js_content)

    print(f"ğŸ“„ å·²ç”Ÿæˆç›®å½• JS æ–‡ä»¶: {page_file}")

print("\nğŸ‰ å®Œæˆï¼")
