import os
import requests
from bs4 import BeautifulSoup
import re
import time
import json

# ================== é…ç½® ==================
TXT_FILE = r"E:\é¢‘é“ç®¡ç†å¤§å…¨\æ¼«ç”»é¢‘é“æ€»ç›®å½•\æ¼«ç”»(å½©è‰²)å…¨é›†\9.html"

# åˆ†é¡µ JS è¾“å‡ºç›®å½•
OUTPUT_DIR_JS = r"E:\é¢‘é“ç®¡ç†å¤§å…¨\comics\category\all_page"
# æ¯ä¸ªæ¼«ç”»ç‹¬ç«‹ JS æ–‡ä»¶ç›®å½•
OUTPUT_COMICS_DIR = r"E:\é¢‘é“ç®¡ç†å¤§å…¨\comics\menu"

os.makedirs(OUTPUT_DIR_JS, exist_ok=True)
os.makedirs(OUTPUT_COMICS_DIR, exist_ok=True)

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Referer": "https://telegra.ph/"
}

ITEMS_PER_PAGE = 100   # æ¯é¡µæ¼«ç”»æ•°é‡
START_PAGE_NUM = 801     # åˆ†é¡µ JS æ–‡ä»¶ç¼–å·èµ·å§‹å€¼
START_IDCODE = 8001       # æ¼«ç”» IDcode èµ·å§‹å€¼

# ================== å·¥å…·å‡½æ•° ==================
def safe_filename(s):
    return re.sub(r'[\\/:*?"<>|]', '_', s)[:150].strip()

def normalize_src(u):
    if not u:
        return None
    u = u.strip()
    if u.startswith("//"):
        u = "https:" + u
    if u.startswith("/file/"):
        u = "https://telegra.ph" + u
    if "telegra.ph/file/" in u and not u.startswith("http"):
        u = "https://" + u
    return u

def extract_title(soup):
    tag = soup.select_one("header.tl_article_header h1") or soup.find("h1") or soup.find("title")
    return tag.get_text(strip=True) if tag else "æœªå‘½åæ¼«ç”»"

def extract_images(soup, html_text):
    imgs = []
    for fig in soup.find_all("figure"):
        img = fig.find("img")
        if img:
            src = img.get("src") or img.get("data-src") or img.get("data-original")
            src = normalize_src(src)
            if src:
                imgs.append(src)
    # æ­£åˆ™å…œåº•
    if not imgs:
        pattern = re.compile(r'<img[^>]+src=["\']([^"\']+)["\']', re.I)
        for m in pattern.finditer(html_text):
            src = normalize_src(m.group(1))
            if src:
                imgs.append(src)
    # å»é‡ä¿æŒé¡ºåº
    seen = set()
    out = []
    for u in imgs:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out

# ================== ä¸»æµç¨‹ ==================
SOURCE_URLS = []

# è¯»å– TXT æ–‡ä»¶å¹¶è§£ææ¯è¡Œ
with open(TXT_FILE, "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        # å°è¯•æå– <a href="URL">æ ‡é¢˜</a>
        match = re.search(r'<a\s+href=["\'](.*?)["\'].*?>(.*?)</a>', line)
        if match:
            url, title_hint = match.groups()
            SOURCE_URLS.append({"url": url.strip(), "title_hint": title_hint.strip()})
        else:
            # çº¯ URL
            SOURCE_URLS.append({"url": line.strip(), "title_hint": None})

comic_list = []

for idx, item in enumerate(SOURCE_URLS):
    current_idcode = START_IDCODE + idx
    url = item["url"]
    title_hint = item["title_hint"]
    print(f"é‡‡é›† IDcode {current_idcode}: {url}")
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = resp.apparent_encoding or "utf-8"
        html = resp.text
        soup = BeautifulSoup(html, "html.parser")

        title = extract_title(soup)
        if title_hint:
            title = title_hint  # ä¼˜å…ˆç”¨ TXT ä¸­çš„æ ‡é¢˜

        imgs = extract_images(soup, html)
        cover_url = imgs[0] if imgs else ""

        # æ„å»ºæ¼«ç”»æ•°æ®å¯¹è±¡
        PAGE_DATA = {
            "IDcode": current_idcode,
            "title": title,
            "cover_url": cover_url,
            "url": f"https://yunvgong.com/comics/comicDetails.html?series={current_idcode}",
            "genres": ["å½©è‰²"]  # æ–°å¢æ ‡ç­¾
        }
        comic_list.append(PAGE_DATA)

        # æ¯ä¸ªæ¼«ç”»ç”Ÿæˆç‹¬ç«‹ JS æ–‡ä»¶
        comic_js_content = f"const comicData = {json.dumps({'IDcode': current_idcode, 'title': title, 'sourceUrl': url, 'images': imgs, 'genres': ['å½©è‰²']}, ensure_ascii=False, indent=4)};\n"
        comic_js_file = os.path.join(OUTPUT_COMICS_DIR, f"{current_idcode}_comics_data.js")
        with open(comic_js_file, "w", encoding="utf-8") as cf:
            cf.write(comic_js_content)

        print(f"í ½í³„ å·²ç”Ÿæˆæ¼«ç”» JS æ–‡ä»¶: {comic_js_file}")
        time.sleep(0.5)
    except Exception as e:
        print(f"âŒ é‡‡é›†å¤±è´¥: {e}")
        continue

# ================== ç”Ÿæˆåˆ†é¡µ JS æ–‡ä»¶ ==================
page_count = (len(comic_list) + ITEMS_PER_PAGE - 1) // ITEMS_PER_PAGE

for page_idx in range(page_count):
    start = page_idx * ITEMS_PER_PAGE
    end = start + ITEMS_PER_PAGE
    page_items = comic_list[start:end]

    current_page_num = START_PAGE_NUM + page_idx

    js_content = "// æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼štitleã€coverã€urlã€genres\n"
    js_content += "window.PAGE_DATA = " + json.dumps(page_items, ensure_ascii=False, indent=4) + ";\n"

    page_file = os.path.join(OUTPUT_DIR_JS, f"page_{current_page_num}.js")
    with open(page_file, "w", encoding="utf-8") as pf:
        pf.write(js_content)

    print(f"í ½í³„ å·²ç”Ÿæˆç›®å½• JS æ–‡ä»¶: {page_file}")

print("\ní ¼í¾‰ å®Œæˆï¼")
